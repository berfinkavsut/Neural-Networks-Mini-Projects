# Mini Projects in Neural Networks

These mini-project assignments were solved for the EEE 443 Neural Network course at Bilkent University during Fall Semester 2020. The assignments were implemented in MATLAB. Sample exercises were provided in Python with Jupyter notebooks. For more detailed information, you can check the assignment files, MATLAB scripts, and solution reports. 

## Assignment #1 
- Neural network with one single hidden layer was implemented for XOR problem.
- Single layer perceptron was coded from scratch for image classification task.
- Two layer neural network was studied from a sample exercise in Jupyter notebook. 

## Assignment #2 
- Stochastic gradient descent on mini-batches was coded from scratch for cat vs. dog classification task. The neural network had two layers. Gradient descent with momentum was added to improve learning process. Test & training squared errors and classification errros were plotted. 
- Neural network with embedding matrix was implemented for one of the natural language processing (NLP) problems from scratch. Embedding matrix was used to map words to a vector representation. Embedding matrix were also learnt by training. Softmax activation function in output layer and cross-entropy loss function was used. 
- Fully connected networks and dropout concepts were studied from a sample exercise in Jupyter notebook. 

## Assignment #3 
- Autoencoder with a single hidden layer was implemented for image data from scratch. Tykhonov regularization (L2 norm regularization) was included to enforce minimization of weight norms. Kullback-Leibler divergence was included to enforce sparsity.
- CNNs and Pytorch framework were studied from sample exercises in Jupyter notebook. 
- Recurrent neural network (RNN) was implemented for a time-series sensor signals data from scratch. Weight initialization with Xavier Uniform distribution was used. 
